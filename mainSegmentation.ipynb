{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b07cf2f-b81d-43ef-907a-c49fed2d95ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torchvision import transforms\n",
    "from progressBar import printProgressBar\n",
    "import matplotlib.pyplot as plt\n",
    "import medicalDataLoader\n",
    "import argparse\n",
    "from utils import *\n",
    "from AttU_Net import *\n",
    "import random\n",
    "import torch\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e976350-3fd5-4406-8511-86a06a9b4494",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "220c7dcc-8438-454d-97b1-8e989a7b8f06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def runTraining():\n",
    "    print('-' * 40)\n",
    "    print('~~~~~~~~  Starting the training... ~~~~~~')\n",
    "    print('-' * 40)\n",
    "\n",
    "    ## DEFINE HYPERPARAMETERS (batch_size > 1)\n",
    "    batch_size = 16\n",
    "    batch_size_val = 4\n",
    "    lr = 0.001    # Learning Rate\n",
    "    epoch = 20 # Number of epochs\n",
    "    \n",
    "    root_dir = './Data/'\n",
    "\n",
    "    print(' Dataset: {} '.format(root_dir))\n",
    "\n",
    "    ## DEFINE THE TRANSFORMATIONS TO DO AND THE VARIABLES FOR TRAINING AND VALIDATION\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    mask_transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    transform1 = transforms.Compose([\n",
    "        transforms.RandomVerticalFlip(p=1.0),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    mask_transform1 = transforms.Compose([\n",
    "        transforms.RandomVerticalFlip(p=1.0),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    transform2 = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=1.0),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    mask_transform2 = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=1.0),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    train_set_full = medicalDataLoader.MedicalImageDataset('train',\n",
    "                                                      root_dir,\n",
    "                                                      transform=transform,\n",
    "                                                      mask_transform=mask_transform,\n",
    "                                                      augment=False,\n",
    "                                                      equalize=False)\n",
    "    \n",
    "    train_set_full1 = medicalDataLoader.MedicalImageDataset('train',\n",
    "                                                      root_dir,\n",
    "                                                      transform=transform1,\n",
    "                                                      mask_transform=mask_transform1,\n",
    "                                                      augment=False,\n",
    "                                                      equalize=False)\n",
    "    \n",
    "    train_set_full2 = medicalDataLoader.MedicalImageDataset('train',\n",
    "                                                      root_dir,\n",
    "                                                      transform=transform2,\n",
    "                                                      mask_transform=mask_transform2,\n",
    "                                                      augment=False,\n",
    "                                                      equalize=False)\n",
    "\n",
    "    concatenated_dataset = ConcatDataset([train_set_full, train_set_full1, train_set_full2])\n",
    "\n",
    "    \n",
    "    train_loader_full = DataLoader(concatenated_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              worker_init_fn=np.random.seed(0),\n",
    "                              num_workers=0,\n",
    "                              shuffle=True)\n",
    "\n",
    "    val_set = medicalDataLoader.MedicalImageDataset('val',\n",
    "                                                    root_dir,\n",
    "                                                    transform=transform,\n",
    "                                                    mask_transform=mask_transform,\n",
    "                                                    equalize=False)\n",
    "\n",
    "    val_loader = DataLoader(val_set,\n",
    "                            batch_size=batch_size_val,\n",
    "                            worker_init_fn=np.random.seed(0),\n",
    "                            num_workers=0,\n",
    "                            shuffle=False)\n",
    "\n",
    "\n",
    "    ## INITIALIZE YOUR MODEL\n",
    "    num_classes = 4 # NUMBER OF CLASSES\n",
    "\n",
    "    print(\"~~~~~~~~~~~ Creating the UNet model ~~~~~~~~~~\")\n",
    "    modelName = 'Attention_U_Net'\n",
    "    print(\" Model Name: {}\".format(modelName))\n",
    "\n",
    "    ## CREATION OF YOUR MODEL\n",
    "    net = AttU_Net(1, num_classes)\n",
    "    #net.load_state_dict(torch.load('./models/AttU_Net_2/model'))\n",
    "\n",
    "    print(\"Total params: {0:,}\".format(sum(p.numel() for p in net.parameters() if p.requires_grad)))\n",
    "\n",
    "    def dice_loss(predictions, targets, smooth=0.0001):\n",
    "        total_loss = 0.0\n",
    "        num_classes = 4\n",
    "        weights = [0.25, 0.25, 0.25, 0.25]\n",
    "        for class_idx in range(num_classes):\n",
    "            class_targets = (targets == class_idx).float()\n",
    "            class_predictions = predictions[:, class_idx, ...]\n",
    "\n",
    "            intersection = torch.sum(class_predictions * class_targets)\n",
    "            union = torch.sum(class_predictions) + torch.sum(class_targets) + smooth\n",
    "            class_dice = (2.0 * intersection + smooth) / union\n",
    "            class_loss = 1.0 - class_dice\n",
    "\n",
    "            total_loss += weights[class_idx]*class_loss\n",
    "\n",
    "        return total_loss\n",
    "    \n",
    "    # DEFINE YOUR OUTPUT COMPONENTS (e.g., SOFTMAX, LOSS FUNCTION, ETC)\n",
    "    softMax = torch.nn.Softmax(dim=1)\n",
    "    weights = torch.tensor([1.0, 3.0, 3.0, 3.0])\n",
    "    CE_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    ## PUT EVERYTHING IN GPU RESOURCES    \n",
    "    if torch.cuda.is_available():\n",
    "        net.cuda()\n",
    "        softMax.cuda()\n",
    "        CE_loss.cuda()\n",
    "        dice_loss.cuda()\n",
    "\n",
    "    ## DEFINE YOUR OPTIMIZER\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=0.0001)\n",
    "\n",
    "    ### To save statistics ####\n",
    "    lossTotalTraining = []\n",
    "    lossTotalVal = []\n",
    "    Best_loss_val = 1000\n",
    "    BestEpoch = 0\n",
    "    \n",
    "    directory = 'Results/Statistics/' + modelName\n",
    "\n",
    "    print(\"~~~~~~~~~~~ Starting the training ~~~~~~~~~~\")\n",
    "    if os.path.exists(directory)==False:\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    ## START THE TRAINING\n",
    "    \n",
    "    ## FOR EACH EPOCH\n",
    "    for i in range(epoch):\n",
    "        net.train()\n",
    "        lossEpoch = []\n",
    "        DSCEpoch = []\n",
    "        DSCEpoch_w = []\n",
    "        num_batches = len(train_loader_full)\n",
    "        ## FOR EACH BATCH\n",
    "        for j, data in enumerate(train_loader_full):\n",
    "            ### Set to zero all the gradients\n",
    "            net.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ## GET IMAGES, LABELS and IMG NAMES\n",
    "            images, labels, img_names = data\n",
    "\n",
    "            ### From numpy to torch variables\n",
    "            labels = to_var(labels)\n",
    "            images = to_var(images)\n",
    "\n",
    "            ################### Train ###################\n",
    "            #-- The CNN makes its predictions (forward pass)\n",
    "            net_predictions = net(images)\n",
    "            #-- Compute the losses --#\n",
    "            # THIS FUNCTION IS TO CONVERT LABELS TO A FORMAT TO BE USED IN THIS CODE\n",
    "            segmentation_classes = getTargetSegmentation(labels)\n",
    "            # COMPUTE THE LOSS\n",
    "            CE_loss_value = CE_loss(softMax(net_predictions), segmentation_classes) # XXXXXX and YYYYYYY are your inputs for the CE\n",
    "            Dice_loss_value = dice_loss(softMax(net_predictions), segmentation_classes)\n",
    "            lossTotal = 0.2*Dice_loss_value+CE_loss_value\n",
    "\n",
    "            # DO THE STEPS FOR BACKPROP (two things to be done in pytorch)\n",
    "            lossTotal.backward()\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # THIS IS JUST TO VISUALIZE THE TRAINING \n",
    "            lossEpoch.append(lossTotal.cpu().data.numpy())\n",
    "            printProgressBar(j + 1, num_batches,\n",
    "                             prefix=\"[Training] Epoch: {} \".format(i),\n",
    "                             length=15,\n",
    "                             suffix=\" Loss: {:.4f}, \".format(lossTotal))\n",
    "\n",
    "        lossEpoch = np.asarray(lossEpoch)\n",
    "        lossEpoch = lossEpoch.mean()\n",
    "\n",
    "        lossTotalTraining.append(lossEpoch)\n",
    "\n",
    "        printProgressBar(num_batches, num_batches,\n",
    "                             done=\"[Training] Epoch: {}, LossG: {:.4f}\".format(i,lossEpoch))\n",
    "        \n",
    "        loss_val = []\n",
    "        net.eval()\n",
    "        for j, data in enumerate(val_loader):\n",
    "            ## GET IMAGES, LABELS and IMG NAMES\n",
    "            images_val, labels_val, img_names = data\n",
    "\n",
    "            ### From numpy to torch variables\n",
    "            labels_val = to_var(labels_val)\n",
    "            images_val = to_var(images_val)\n",
    "            \n",
    "            pred = net(images_val)\n",
    "            segmentation_classes_val = getTargetSegmentation(labels_val)\n",
    "            \n",
    "            CE_loss_value = CE_loss(softMax(pred), segmentation_classes_val) # XXXXXX and YYYYYYY are your inputs for the CE\n",
    "            Dice_loss_value = dice_loss(softMax(pred), segmentation_classes_val)\n",
    "            loss_val.append((0.2*Dice_loss_value+CE_loss_value).cpu().data.numpy())\n",
    "        loss_val = np.asarray(loss_val).mean()\n",
    "        print(\"Loss validation : \" + str(loss_val.item()))\n",
    "        \n",
    "        \n",
    "        ## THIS IS HOW YOU WILL SAVE THE TRAINED MODELS AFTER EACH EPOCH. \n",
    "        ## WARNING!!!!! YOU DON'T WANT TO SAVE IT AT EACH EPOCH, BUT ONLY WHEN THE MODEL WORKS BEST ON THE VALIDATION SET!!\n",
    "        if not os.path.exists('./models/' + modelName):\n",
    "                os.makedirs('./models/' + modelName)\n",
    "        if loss_val < Best_loss_val:\n",
    "            Best_loss_val = loss_val\n",
    "            BestEpoch = i\n",
    "            torch.save(net.state_dict(), './models/' + modelName + '/model')\n",
    "            \n",
    "        np.save(os.path.join(directory, 'Losses.npy'), lossTotalTraining)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67f81713-4c18-4dd3-bc72-35dd8f30a339",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "~~~~~~~~  Starting the training... ~~~~~~\n",
      "----------------------------------------\n",
      " Dataset: ./Data/ \n",
      "~~~~~~~~~~~ Creating the UNet model ~~~~~~~~~~\n",
      " Model Name: Attention_U_Net\n",
      "Total params: 2,184,504\n",
      "~~~~~~~~~~~ Starting the training ~~~~~~~~~~\n",
      "[Training] Epoch: 0 [DONE]                                 \n",
      "[Training] Epoch: 0, LossG: 1.3389                                                                           \n",
      "Loss validation : 1.3399007320404053\n",
      "[Training] Epoch: 1 [DONE]                                 \n",
      "[Training] Epoch: 1, LossG: 1.1810                                                                           \n",
      "Loss validation : 1.146612524986267\n",
      "[Training] Epoch: 2 [DONE]                                 \n",
      "[Training] Epoch: 2, LossG: 1.0749                                                                           \n",
      "Loss validation : 1.036014199256897\n",
      "[Training] Epoch: 3 [DONE]                                 \n",
      "[Training] Epoch: 3, LossG: 1.0067                                                                           \n",
      "Loss validation : 0.9907001852989197\n",
      "[Training] Epoch: 4 [DONE]                                 \n",
      "[Training] Epoch: 4, LossG: 0.9676                                                                           \n",
      "Loss validation : 0.9644377827644348\n",
      "[Training] Epoch: 5 [DONE]                                 \n",
      "[Training] Epoch: 5, LossG: 0.9424                                                                           \n",
      "Loss validation : 0.9345099925994873\n",
      "[Training] Epoch: 6 [DONE]                                 \n",
      "[Training] Epoch: 6, LossG: 0.9241                                                                           \n",
      "Loss validation : 0.9352928996086121\n",
      "[Training] Epoch: 7 [DONE]                                 \n",
      "[Training] Epoch: 7, LossG: 0.9110                                                                           \n",
      "Loss validation : 0.9339791536331177\n",
      "[Training] Epoch: 8 [DONE]                                 \n",
      "[Training] Epoch: 8, LossG: 0.9027                                                                           \n",
      "Loss validation : 0.9077656269073486\n",
      "[Training] Epoch: 9 [DONE]                                 \n",
      "[Training] Epoch: 9, LossG: 0.8996                                                                           \n",
      "Loss validation : 0.9087037444114685\n",
      "[Training] Epoch: 10 [DONE]                                 \n",
      "[Training] Epoch: 10, LossG: 0.8777                                                                          \n",
      "Loss validation : 0.9413734078407288\n",
      "[Training] Epoch: 11 [DONE]                                 \n",
      "[Training] Epoch: 11, LossG: 0.8565                                                                          \n",
      "Loss validation : 0.910519540309906\n",
      "[Training] Epoch: 12 [DONE]                                 \n",
      "[Training] Epoch: 12, LossG: 0.8334                                                                          \n",
      "Loss validation : 0.828369140625\n",
      "[Training] Epoch: 13 [DONE]                                 \n",
      "[Training] Epoch: 13, LossG: 0.8099                                                                          \n",
      "Loss validation : 0.8533326983451843\n",
      "[Training] Epoch: 14 [DONE]                                 \n",
      "[Training] Epoch: 14, LossG: 0.8020                                                                          \n",
      "Loss validation : 0.8682686686515808\n",
      "[Training] Epoch: 15 [DONE]                                 \n",
      "[Training] Epoch: 15, LossG: 0.7976                                                                          \n",
      "Loss validation : 0.8154235482215881\n",
      "[Training] Epoch: 16 [DONE]                                 \n",
      "[Training] Epoch: 16, LossG: 0.7958                                                                          \n",
      "Loss validation : 0.8038029670715332\n",
      "[Training] Epoch: 17 [DONE]                                 \n",
      "[Training] Epoch: 17, LossG: 0.7876                                                                          \n",
      "Loss validation : 0.8012443780899048\n",
      "[Training] Epoch: 18 [DONE]                                 \n",
      "[Training] Epoch: 18, LossG: 0.7838                                                                          \n",
      "Loss validation : 0.8174630999565125\n",
      "[Training] Epoch: 19 [DONE]                                 \n",
      "[Training] Epoch: 19, LossG: 0.7787                                                                          \n",
      "Loss validation : 0.825873851776123\n"
     ]
    }
   ],
   "source": [
    "runTraining()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae10184-bacf-4c4d-9767-3272a76a0052",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b08deb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
